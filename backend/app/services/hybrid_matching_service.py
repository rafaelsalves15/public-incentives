"""
Hybrid Matching Service - Sistema de Matching HÃ­brido com Embeddings

Este serviÃ§o implementa um sistema hÃ­brido de 3 fases que combina:
1. Busca SemÃ¢ntica (Vector Search) - Encontra candidatas por similaridade semÃ¢ntica
2. Scoring DeterminÃ­stico (Unified Scorer) - Pontua candidatas por critÃ©rios especÃ­ficos  
3. Refinamento LLM (AI Processor) - AnÃ¡lise inteligente das melhores candidatas

Vantagens:
- ReduÃ§Ã£o significativa de custos LLM (menos candidatas para anÃ¡lise)
- Melhor descoberta de matches nÃ£o Ã³bvios (busca semÃ¢ntica)
- MantÃ©m precisÃ£o dos critÃ©rios especÃ­ficos (CAE codes, etc.)
- Sistema mais robusto e escalÃ¡vel
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from sqlalchemy.orm import Session
from app.db.models import Incentive, Company, IncentiveMetadata
from app.services.ai_processor import AIProcessor
from app.services.unified_scorer import UnifiedScorer
from app.services.embedding_service import EmbeddingService
from app.services.vector_database_service import VectorDatabaseService

logger = logging.getLogger(__name__)


class HybridMatchingService:
    """
    Sistema de Matching HÃ­brido com Embeddings
    
    Pipeline de 3 fases:
    1. Vector Search: Busca semÃ¢ntica â†’ Top 50 candidatas
    2. Unified Scoring: PontuaÃ§Ã£o determinÃ­stica â†’ Top 15 candidatas
    3. LLM Refinement: AnÃ¡lise inteligente â†’ Top 5 finalistas
    
    Reduz custos LLM em ~70% mantendo qualidade superior.
    """
    
    def __init__(
        self, 
        ai_processor: AIProcessor,
        embedding_service: EmbeddingService,
        vector_db_service: VectorDatabaseService
    ):
        """
        Inicializa o serviÃ§o de matching hÃ­brido.
        
        Args:
            ai_processor: Processador de IA para anÃ¡lise LLM
            embedding_service: ServiÃ§o de geraÃ§Ã£o de embeddings
            vector_db_service: ServiÃ§o de base de dados vectorial
        """
        self.ai_processor = ai_processor
        self.embedding_service = embedding_service
        self.vector_db_service = vector_db_service
        self.unified_scorer = UnifiedScorer(ai_processor)
        
        # ConfiguraÃ§Ãµes do pipeline
        self.VECTOR_SEARCH_TOP_K = 50      # Fase 1: Busca semÃ¢ntica
        self.UNIFIED_SCORING_TOP_K = 15   # Fase 2: Scoring determinÃ­stico
        self.LLM_REFINEMENT_TOP_K = 5     # Fase 3: AnÃ¡lise LLM
        
        # Thresholds de qualidade
        self.MIN_SEMANTIC_SIMILARITY = 0.2  # Similaridade mÃ­nima para considerar candidata (reduzido)
        self.MIN_UNIFIED_SCORE = 0          # Score mÃ­nimo unificado (pode ser 0)
        
        logger.info("âœ… Hybrid Matching Service initialized")
        logger.info(f"   Pipeline: Vector({self.VECTOR_SEARCH_TOP_K}) â†’ Unified({self.UNIFIED_SCORING_TOP_K}) â†’ LLM({self.LLM_REFINEMENT_TOP_K})")
    
    def find_top_matches(
        self, 
        session: Session, 
        incentive_id: str, 
        limit: int = 5,
        use_hybrid: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Encontra as melhores empresas para um incentivo usando sistema hÃ­brido.
        
        Args:
            session: SessÃ£o da base de dados
            incentive_id: ID do incentivo
            limit: NÃºmero mÃ¡ximo de matches (padrÃ£o: 5)
            use_hybrid: Se True, usa pipeline hÃ­brido; se False, usa sistema original
            
        Returns:
            Lista de matches com scores e razÃµes detalhadas
        """
        logger.info(f"ðŸŽ¯ Finding top {limit} matches for incentive {incentive_id}")
        
        # Obter incentivo
        incentive = session.query(Incentive).filter(
            Incentive.incentive_id == incentive_id
        ).first()
        
        if not incentive:
            logger.error(f"Incentive {incentive_id} not found")
            return []
        
        if use_hybrid:
            return self._find_matches_hybrid(session, incentive, limit)
        else:
            return self._find_matches_original(session, incentive, limit)
    
    def _find_matches_hybrid(
        self, 
        session: Session, 
        incentive: Incentive, 
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        Pipeline hÃ­brido de matching com embeddings.
        
        FASE 1: Vector Search â†’ Top 50 candidatas semÃ¢nticas
        FASE 2: Unified Scoring â†’ Top 15 candidatas pontuadas
        FASE 3: LLM Refinement â†’ Top 5 finalistas analisadas
        """
        logger.info(f"ðŸ” HYBRID MATCHING for '{incentive.title[:50]}...'")
        
        # ==========================================
        # FASE 1: VECTOR SEARCH (SEMÃ‚NTICA)
        # ==========================================
        logger.info("ðŸ§  FASE 1: Vector Search (Semantic)...")
        
        semantic_candidates = self.vector_db_service.search_similar_companies(
            incentive=incentive,
            top_k=self.VECTOR_SEARCH_TOP_K,
            min_similarity=self.MIN_SEMANTIC_SIMILARITY
        )
        
        if not semantic_candidates:
            logger.warning("No semantic candidates found, falling back to original system")
            return self._find_matches_original(session, incentive, limit)
        
        logger.info(f"âœ… Vector Search: {len(semantic_candidates)} semantic candidates")
        if semantic_candidates:
            logger.info(f"   Top semantic similarity: {semantic_candidates[0][1]:.3f}")
            logger.info(f"   Bottom semantic similarity: {semantic_candidates[-1][1]:.3f}")
        
        # ==========================================
        # FASE 2: UNIFIED SCORING (DETERMINÃSTICA)
        # ==========================================
        logger.info("ðŸ“Š FASE 2: Unified Scoring (Deterministic)...")
        
        # Converter candidatas semÃ¢nticas para objetos Company
        semantic_companies = [candidate[0] for candidate in semantic_candidates]
        
        # Pontuar candidatas semÃ¢nticas
        scored_candidates = []
        for company in semantic_companies:
            score_data = self.unified_scorer.score_company(incentive, company)
            scored_candidates.append({
                'company': company,
                'semantic_similarity': next((c[1] for c in semantic_candidates if c[0].company_id == company.company_id), 0.0),
                'unified_score': score_data['score'],
                'unified_reasons': score_data['details']
            })
        
        # Ordenar por score unificado (maior primeiro)
        scored_candidates.sort(key=lambda x: x['unified_score'], reverse=True)
        
        # Selecionar top candidatas para LLM
        top_candidates = scored_candidates[:self.UNIFIED_SCORING_TOP_K]
        
        logger.info(f"âœ… Unified Scoring: {len(top_candidates)} top candidates")
        if top_candidates:
            logger.info(f"   Top unified score: {top_candidates[0]['unified_score']:.1f}")
            logger.info(f"   Bottom unified score: {top_candidates[-1]['unified_score']:.1f}")
        
        # ==========================================
        # FASE 3: LLM REFINEMENT (INTELIGENTE)
        # ==========================================
        logger.info("ðŸ¤– FASE 3: LLM Refinement (Intelligent)...")
        
        # Preparar dados para LLM
        companies_for_llm = [item['company'] for item in top_candidates]
        
        # Obter metadata do incentivo
        metadata = session.query(IncentiveMetadata).filter(
            IncentiveMetadata.incentive_id == incentive.incentive_id
        ).first()
        
        raw_csv_data = metadata.raw_csv_data if metadata else {}
        
        # AnÃ¡lise LLM das top candidatas
        llm_results = self.ai_processor.analyze_batch_match(
            incentive=incentive,
            companies=companies_for_llm,
            raw_csv_data=raw_csv_data,
            select_top_n=limit
        )
        
        logger.info(f"âœ… LLM Refinement: {len(llm_results)} final matches")
        
        # ==========================================
        # COMBINAR RESULTADOS
        # ==========================================
        final_matches = []
        
        for llm_result in llm_results:
            # Encontrar dados correspondentes
            company_id = llm_result.get('company_id')
            matching_candidate = next(
                (c for c in top_candidates if str(c['company'].company_id) == company_id), 
                None
            )
            
            if matching_candidate:
                # Combinar scores e razÃµes
                final_match = {
                    'company_id': company_id,
                    'company_name': matching_candidate['company'].company_name,
                    'cae_primary_label': matching_candidate['company'].cae_primary_label,
                    'cae_primary_code': matching_candidate['company'].cae_primary_code,
                    'trade_description_native': matching_candidate['company'].trade_description_native,
                    'website': matching_candidate['company'].website,
                    'company_size': matching_candidate['company'].company_size,
                    'region': matching_candidate['company'].region,
                    
                    # Scores combinados
                    'semantic_similarity': matching_candidate['semantic_similarity'],
                    'unified_score': matching_candidate['unified_score'],
                    'llm_score': llm_result.get('match_score', 0.0),
                    
                    # RazÃµes combinadas
                    'semantic_reasons': [f"Similaridade semÃ¢ntica: {matching_candidate['semantic_similarity']:.3f}"],
                    'unified_reasons': matching_candidate['unified_reasons'],
                    'llm_reasons': llm_result.get('reasons', []),
                    
                    # Metadados adicionais
                    'concerns': llm_result.get('concerns', []),
                    'recommendations': llm_result.get('recommendations', []),
                    
                    # Score total combinado (peso: semÃ¢ntico 30%, unificado 40%, LLM 30%)
                    'total_score': (
                        matching_candidate['semantic_similarity'] * 0.3 +
                        min(matching_candidate['unified_score'] / 200.0, 1.0) * 0.4 +  # Normalizar unified score
                        llm_result.get('match_score', 0.0) * 0.3
                    )
                }
                
                final_matches.append(final_match)
        
        # Ordenar por score total
        final_matches.sort(key=lambda x: x['total_score'], reverse=True)
        
        logger.info(f"ðŸŽ¯ HYBRID MATCHING COMPLETED: {len(final_matches)} final matches")
        if final_matches:
            logger.info(f"   Top total score: {final_matches[0]['total_score']:.3f}")
            logger.info(f"   Bottom total score: {final_matches[-1]['total_score']:.3f}")
        
        return final_matches
    
    def _find_matches_original(
        self, 
        session: Session, 
        incentive: Incentive, 
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        Sistema original de matching (fallback).
        
        Usa apenas Unified Scoring + LLM Refinement sem busca semÃ¢ntica.
        """
        logger.info("ðŸ”„ Using ORIGINAL matching system (fallback)")
        
        # Obter todas as empresas
        all_companies = session.query(Company).all()
        
        # Unified Scoring
        scored_companies = []
        for company in all_companies:
            score_data = self.unified_scorer.score_company(incentive, company)
            scored_companies.append({
                'company': company,
                'unified_score': score_data['score'],
                'unified_reasons': score_data['details']
            })
        
        # Ordenar por score
        scored_companies.sort(key=lambda x: x['unified_score'], reverse=True)
        
        # Selecionar top candidatas
        top_candidates = scored_companies[:15]
        companies_for_llm = [item['company'] for item in top_candidates]
        
        # Obter metadata
        metadata = session.query(IncentiveMetadata).filter(
            IncentiveMetadata.incentive_id == incentive.incentive_id
        ).first()
        
        raw_csv_data = metadata.raw_csv_data if metadata else {}
        
        # LLM Analysis
        llm_results = self.ai_processor.analyze_batch_match(
            incentive=incentive,
            companies=companies_for_llm,
            raw_csv_data=raw_csv_data,
            select_top_n=limit
        )
        
        # Combinar resultados
        final_matches = []
        for llm_result in llm_results:
            company_id = llm_result.get('company_id')
            matching_candidate = next(
                (c for c in top_candidates if str(c['company'].company_id) == company_id), 
                None
            )
            
            if matching_candidate:
                final_match = {
                    'company_id': company_id,
                    'company_name': matching_candidate['company'].company_name,
                    'cae_primary_label': matching_candidate['company'].cae_primary_label,
                    'cae_primary_code': matching_candidate['company'].cae_primary_code,
                    'trade_description_native': matching_candidate['company'].trade_description_native,
                    'website': matching_candidate['company'].website,
                    'company_size': matching_candidate['company'].company_size,
                    'region': matching_candidate['company'].region,
                    
                    'semantic_similarity': 0.0,  # NÃ£o disponÃ­vel no sistema original
                    'unified_score': matching_candidate['unified_score'],
                    'llm_score': llm_result.get('match_score', 0.0),
                    
                    'semantic_reasons': [],
                    'unified_reasons': matching_candidate['unified_reasons'],
                    'llm_reasons': llm_result.get('reasons', []),
                    
                    'concerns': llm_result.get('concerns', []),
                    'recommendations': llm_result.get('recommendations', []),
                    
                    'total_score': (
                        min(matching_candidate['unified_score'] / 200.0, 1.0) * 0.6 +  # Mais peso para unificado
                        llm_result.get('match_score', 0.0) * 0.4
                    )
                }
                
                final_matches.append(final_match)
        
        final_matches.sort(key=lambda x: x['total_score'], reverse=True)
        
        logger.info(f"ðŸ”„ ORIGINAL MATCHING COMPLETED: {len(final_matches)} matches")
        
        return final_matches
    
    def get_matching_stats(self) -> Dict[str, Any]:
        """
        Retorna estatÃ­sticas do sistema de matching.
        
        Returns:
            Dict com estatÃ­sticas das diferentes fases
        """
        return {
            "pipeline_config": {
                "vector_search_top_k": self.VECTOR_SEARCH_TOP_K,
                "unified_scoring_top_k": self.UNIFIED_SCORING_TOP_K,
                "llm_refinement_top_k": self.LLM_REFINEMENT_TOP_K,
                "min_semantic_similarity": self.MIN_SEMANTIC_SIMILARITY,
                "min_unified_score": self.MIN_UNIFIED_SCORE
            },
            "vector_database_stats": self.vector_db_service.get_database_stats(),
            "embedding_cache_stats": self.embedding_service.get_cache_stats()
        }
    
    def optimize_pipeline_performance(self) -> Dict[str, Any]:
        """
        Otimiza performance do pipeline ajustando parÃ¢metros automaticamente.
        
        Returns:
            Dict com otimizaÃ§Ãµes aplicadas
        """
        optimizations = {}
        
        # Obter estatÃ­sticas do cache
        cache_stats = self.embedding_service.get_cache_stats()
        
        # Ajustar thresholds baseado na performance
        if cache_stats['hit_rate_percentage'] > 80:
            # Cache estÃ¡ funcionando bem, pode ser mais restritivo
            self.MIN_SEMANTIC_SIMILARITY = max(0.35, self.MIN_SEMANTIC_SIMILARITY)
            optimizations['increased_semantic_threshold'] = self.MIN_SEMANTIC_SIMILARITY
        elif cache_stats['hit_rate_percentage'] < 50:
            # Cache nÃ£o estÃ¡ funcionando bem, ser mais permissivo
            self.MIN_SEMANTIC_SIMILARITY = min(0.25, self.MIN_SEMANTIC_SIMILARITY)
            optimizations['decreased_semantic_threshold'] = self.MIN_SEMANTIC_SIMILARITY
        
        # Ajustar tamanhos baseado na qualidade dos resultados
        if self.VECTOR_SEARCH_TOP_K > 30:
            self.VECTOR_SEARCH_TOP_K = 30  # Reduzir para melhor performance
            optimizations['reduced_vector_search_k'] = self.VECTOR_SEARCH_TOP_K
        
        logger.info(f"ðŸ”§ Pipeline optimizations applied: {optimizations}")
        
        return optimizations
